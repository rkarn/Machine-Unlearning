{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using another approach where the architecture of the network is constant while learning each task. We use the method called \"Elastic Weight Consolidation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the source code from https://github.com/ganguli-lab/pathint \n",
    "Put this jupytper notebook into `fig_split_mnist` folder. Then run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "graph_replace = tf.contrib.graph_editor.graph_replace\n",
    "\n",
    "import sys, os\n",
    "sys.path.extend([os.path.expanduser('..')])\n",
    "from pathint import utils\n",
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "# import operator\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = tf.select if hasattr(tf, 'select') else tf.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "# Network params\n",
    "n_hidden_units = 50\n",
    "activation_fn = tf.nn.relu\n",
    "\n",
    "# Optimization params\n",
    "batch_size = 1000\n",
    "epochs_per_task = 5\n",
    "\n",
    "n_stats = 1\n",
    "\n",
    "# Reset optimizer after each age\n",
    "reset_optimizer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_labels = [[0,1], [2,3], [4,5], [6,7], [8,9]]\n",
    "#task_labels = [[0,1], [2,3], [4,5], [6,7], [8,9], [4,6],[8,1],[0,3],[2,4],[5,7]]\n",
    "#task_labels = [[0,1], [2,3], [4,5], [6,7], [8,9], [4,6],[8,1],[0,3],[2,9],[5,7]]\n",
    "# task_labels = [[0,1,2,3,4], [5,6,7,8,9]]\n",
    "n_tasks = len(task_labels)\n",
    "training_datasets = utils.construct_split_mnist(task_labels, split='train')\n",
    "validation_datasets = utils.construct_split_mnist(task_labels, split='test')\n",
    "# training_datasets = utils.mk_training_validation_splits(full_datasets, split_fractions=(0.9, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct network, loss, and updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.equal(output_mask[None, :], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import keras.activations as activations\n",
    "\n",
    "output_mask = tf.Variable(tf.zeros(output_dim), name=\"mask\", trainable=False)\n",
    "\n",
    "def masked_softmax(logits):\n",
    "    # logits are [batch_size, output_dim]\n",
    "    x = select(tf.tile(tf.equal(output_mask[None, :], 1.0), [tf.shape(logits)[0], 1]), logits, -1e32 * tf.ones_like(logits))\n",
    "    return activations.softmax(x)\n",
    "\n",
    "def set_active_outputs(labels):\n",
    "    new_mask = np.zeros(output_dim)\n",
    "    for l in labels:\n",
    "        new_mask[l] = 1.0\n",
    "    sess.run(output_mask.assign(new_mask))\n",
    "    #print(sess.run(output_mask))\n",
    "    \n",
    "def masked_predict(model, data, targets):\n",
    "    pred = model.predict(data)\n",
    "    #print(pred)\n",
    "    acc = np.argmax(pred,1)==np.argmax(targets,1)\n",
    "    return acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "def custom_activation(x):\n",
    "    A=K.relu(x)\n",
    "    return(K.log(1+A))\n",
    "get_custom_objects().update({'custom_activation':Activation(custom_activation)})\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_units, activation=activation_fn, input_shape=(input_dim,)))\n",
    "#model.add(Dense(n_hidden_units, activation=activation_fn))\n",
    "model.add(Dense(output_dim, kernel_initializer='zero', activation=masked_softmax))\n",
    "#model.add(Dense(output_dim, activation=masked_softmax, input_shape=(input_dim,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:2747: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from pathint import protocols\n",
    "from pathint.optimizers import KOOptimizer\n",
    "from keras.optimizers import Adam, RMSprop,SGD\n",
    "from keras.callbacks import Callback\n",
    "from pathint.keras_utils import LossHistory\n",
    "from keras.callbacks import History \n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "#protocol_name, protocol = protocols.PATH_INT_PROTOCOL(omega_decay='sum',xi=1e-3)\n",
    "protocol_name, protocol = protocols.PATH_INT_PROTOCOL(omega_decay='sum',xi=1e-3)\n",
    "#protocol_name, protocol = protocols.FISHER_PROTOCOL('sum')\n",
    "opt = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "# opt = SGD(1e-3)\n",
    "# opt = RMSprop(lr=1e-3)\n",
    "oopt = KOOptimizer(opt, model=model, compute_fisher=False, **protocol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=oopt, metrics=['accuracy'])\n",
    "model.model._make_train_function()\n",
    "saved_weights = model.get_weights()\n",
    "\n",
    "save_weights_epoch=[]\n",
    "save_loss_epoch=[]\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: save_weights_epoch.append(model.get_weights()))\n",
    "history = LossHistory()\n",
    "callbacks = [history]\n",
    "datafile_name = \"split_mnist_data_%s.pkl.gz\"%protocol_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "def run_fits(cvals, training_data, valid_data, eval_on_train_set=False, nstats=1):\n",
    "    acc_mean = dict()\n",
    "    acc_std = dict()\n",
    "    model_weights_save = []   #Empty list to save the model weights aftertraining each task\n",
    "    for cidx, cval_ in enumerate(tqdm(cvals)):\n",
    "        runs = []\n",
    "        for runid in range(nstats):\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # model.set_weights(saved_weights)\n",
    "            cstuffs = []\n",
    "            evals = []\n",
    "            print(\"setting cval\")\n",
    "            cval = cval_\n",
    "            oopt.set_strength(cval)\n",
    "            oopt.init_task_vars()\n",
    "            print(\"cval is\", sess.run(oopt.lam))\n",
    "            for age, tidx in enumerate(range(n_tasks)):\n",
    "                print(\"Task %i\"%(age))\n",
    "                set_active_outputs(task_labels[age])\n",
    "                stuffs = model.fit(training_data[tidx][0], training_data[tidx][1], batch_size, epochs_per_task, callbacks=[history,print_weights], verbose=0)\n",
    "                oopt.update_task_metrics(training_data[tidx][0], training_data[tidx][1], batch_size)\n",
    "                oopt.update_task_vars()\n",
    "                ftask = []\n",
    "                model_weights_save.append(model.get_weights()) #Save the model weights aftertraining each task\n",
    "                for j in range(n_tasks):\n",
    "                    set_active_outputs(task_labels[j])\n",
    "                    if eval_on_train_set:\n",
    "                        f_ = masked_predict(model, training_data[j][0], training_data[j][1])\n",
    "                    else:\n",
    "                        f_ = masked_predict(model, valid_data[j][0], valid_data[j][1])\n",
    "                    ftask.append(np.mean(f_))\n",
    "                print(\"Accuracy\", ftask)\n",
    "                evals.append(ftask)\n",
    "                cstuffs.append(stuffs)\n",
    "\n",
    "                # Re-initialize optimizater variables\n",
    "                if reset_optimizer:\n",
    "                    oopt.reset_optimizer()\n",
    "\n",
    "            evals = np.array(evals)\n",
    "            runs.append(evals)\n",
    "        \n",
    "        runs = np.array(runs)\n",
    "        acc_mean[cval_] = runs.mean(0)\n",
    "        acc_std[cval_] = runs.std(0)\n",
    "    return dict(mean=acc_mean, std=acc_std),model_weights_save,cstuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "# cvals = np.concatenate(([0], np.logspace(-2, 2, 10)))\n",
    "# cvals = np.concatenate(([0], np.logspace(-1, 2, 2)))\n",
    "# cvals = np.concatenate(([0], np.logspace(-2, 0, 3)))\n",
    "cvals = np.logspace(-3, 3, 7)#[0, 1.0, 2, 5, 10]\n",
    "cvals = [1.0]\n",
    "print(cvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting cval\n",
      "cval is 1.0\n",
      "Task 0\n",
      "Accuracy [0.9995271867612293, 0.5053868756121449, 0.5240128068303095, 0.4823766364551863, 0.49117498739283916]\n",
      "Task 1\n",
      "Accuracy [0.9995271867612293, 0.9701273261508325, 0.5240128068303095, 0.4823766364551863, 0.49117498739283916]\n",
      "Task 2\n",
      "Accuracy [0.9995271867612293, 0.910871694417238, 0.9754535752401281, 0.4823766364551863, 0.49117498739283916]\n",
      "Task 3\n",
      "Accuracy [0.9995271867612293, 0.9079333986287953, 0.971718249733191, 0.9879154078549849, 0.49117498739283916]\n",
      "Task 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy [0.9990543735224586, 0.9094025465230167, 0.9791889007470651, 0.9884189325276939, 0.9652042360060514]\n",
      "Task  {0}  accuracy:  0.9990543735224586\n",
      "Task  {1}  accuracy:  0.9094025465230167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  {2}  accuracy:  0.9791889007470651\n",
      "Task  {3}  accuracy:  0.9884189325276939\n",
      "Task  {4}  accuracy:  0.9652042360060514\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "recompute_data = True\n",
    "if recompute_data:\n",
    "    data,model_weights_save,cstuffs = run_fits(cvals, training_datasets, validation_datasets, eval_on_train_set=False, nstats=n_stats)\n",
    "    utils.save_zipped_pickle(data, datafile_name)\n",
    "    \n",
    "for task_id in range(len(task_labels)):\n",
    "    set_active_outputs(task_labels[task_id])    \n",
    "    print('Task ', {task_id}, ' accuracy: ', masked_predict(model, validation_datasets[task_id][0], validation_datasets[task_id][1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlearning task 1 containing labels [2,3]. Please note that the task numbering starts from 0.\n",
    "\n",
    "Zeroing only output layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  {0}  accuracy:  0.9990543735224586\n",
      "Task  {1}  accuracy:  0.5053868756121449\n",
      "Task  {2}  accuracy:  0.9791889007470651\n",
      "Task  {3}  accuracy:  0.9884189325276939\n",
      "Task  {4}  accuracy:  0.9652042360060514\n"
     ]
    }
   ],
   "source": [
    "#Accuracy aftering forgetting one of the task\n",
    "#Assume that task 1 is forgetted.\n",
    "\n",
    "forget_task=1\n",
    "class_to_forget = task_labels[forget_task]\n",
    "#Zero out the weights corresponding to this class\n",
    "for task_id in range(len(task_labels)):\n",
    "    set_active_outputs(task_labels[task_id])\n",
    "    \n",
    "    for k,layer in enumerate(model.layers):  # Exclude output layer\n",
    "        if isinstance(layer, Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            if layer == model.layers[-1]:\n",
    "                for cl in class_to_forget:\n",
    "                    weights[:, cl] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "                    biases[cl] = 0\n",
    "            layer.set_weights([weights, biases])\n",
    "    \n",
    "    print('Task ', {task_id}, ' accuracy: ', masked_predict(model, validation_datasets[task_id][0], validation_datasets[task_id][1]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows the forgetting for task 1 as its accuracy is close to 50%. Other tasks accuracy is unimpacted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-id for last epoch of each task: [64, 129, 189, 254, 314]\n",
      "*----------------------------*\n",
      "Parameter importance (big_omega) for task 0 shape is  [(50,), (10,), (50, 10), (784, 50)]\n",
      "Parameter importance (big_omega) for task 1 shape is  [(50,), (10,), (50, 10), (784, 50)]\n",
      "Parameter importance (big_omega) for task 2 shape is  [(50,), (10,), (50, 10), (784, 50)]\n",
      "Parameter importance (big_omega) for task 3 shape is  [(50,), (10,), (50, 10), (784, 50)]\n",
      "Parameter importance (big_omega) for task 4 shape is  [(50,), (10,), (50, 10), (784, 50)]\n",
      "*----------------------------*\n",
      "Number of non-zero elements of importance (big_omega) for task 0 is  0\n",
      "Number of non-zero elements of importance (big_omega) for task 1 is  30426\n",
      "Number of non-zero elements of importance (big_omega) for task 2 is  33539\n",
      "Number of non-zero elements of importance (big_omega) for task 3 is  34353\n",
      "Number of non-zero elements of importance (big_omega) for task 4 is  36096\n"
     ]
    }
   ],
   "source": [
    "#Inspection of behavior of importance parameter big_omega after every task \n",
    "from numpy import count_nonzero\n",
    "last_epoch_batchindex = []\n",
    "counter = 0\n",
    "for i,j in enumerate(history.batchindex):\n",
    "    try:\n",
    "        if history.batchindex[i+1] > j:\n",
    "            pass\n",
    "        else:\n",
    "            counter = counter+1\n",
    "            if counter%epochs_per_task == 0:\n",
    "                last_epoch_batchindex.append(i)\n",
    "    except:\n",
    "        last_epoch_batchindex.append(i)\n",
    "        \n",
    "\n",
    "print('Batch-id for last epoch of each task:',last_epoch_batchindex)\n",
    "print('*----------------------------*')\n",
    "for i,epoch_id in enumerate(last_epoch_batchindex):\n",
    "    key = list(history.big_omega[epoch_id].keys())\n",
    "    print('Parameter importance (big_omega) for task {0} shape is '.format(i), [history.big_omega[epoch_id][ke].shape for ke in key]) \n",
    "\n",
    "non_zero_imp_param = []\n",
    "print('*----------------------------*')\n",
    "for i,epoch_id in enumerate(last_epoch_batchindex):\n",
    "    key = list(history.big_omega[epoch_id].keys())\n",
    "    non_zero_imp_param.append(sum([count_nonzero(history.big_omega[epoch_id][ke]) for ke in key]))\n",
    "    print('Number of non-zero elements of importance (big_omega) for task {0} is '.format(i),non_zero_imp_param[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_zero_indices(array):\n",
    "    zero_indices = []\n",
    "    if isinstance(array, np.ndarray):\n",
    "        zero_indices = np.argwhere(array == 0).tolist()\n",
    "    else:\n",
    "        if isinstance(array, (list, tuple)):\n",
    "            for i in range(len(array)):\n",
    "                if isinstance(array[i], (list, tuple)):\n",
    "                    for j in range(len(array[i])):\n",
    "                        if array[i][j] == 0:\n",
    "                            zero_indices.append((i, j))\n",
    "                else:\n",
    "                    if array[i] == 0:\n",
    "                        zero_indices.append((i,))\n",
    "    return zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/bias:0' shape=(50,) dtype=float32_ref>, <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'dense_2/kernel:0' shape=(50, 10) dtype=float32_ref>, <tf.Variable 'dense_1/kernel:0' shape=(784, 50) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "Importance_collect= {}\n",
    "for i,epoch_id in enumerate(last_epoch_batchindex):\n",
    "    key = list(history.big_omega[epoch_id].keys())\n",
    "    Importance_collect[i] = [history.big_omega[epoch_id][ke] for ke in key]\n",
    "    \n",
    "print(key)\n",
    "\n",
    "weight_indices_zero = {}\n",
    "for i in range(1,len(last_epoch_batchindex)): #after frist task\n",
    "    weight_indices_zero[i] = []\n",
    "    for j in range(len(Importance_collect[i])):\n",
    "        weight_indices_zero[i].append(find_zero_indices(Importance_collect[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-35-5b9edfeae5c1>(18)<module>()\n",
      "-> for iids in weight_indices_zero[forget_task]:\n",
      "(Pdb) p iids\n",
      "[[2], [3], [4], [5], [6], [7], [8], [9]]\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5b9edfeae5c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0miids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_indices_zero\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforget_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-5b9edfeae5c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0miids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_indices_zero\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforget_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Accuracy aftering forgetting one of the task\n",
    "#Assume that task 1 is forgetted.\n",
    "\n",
    "forget_task=1\n",
    "class_to_forget = task_labels[forget_task]\n",
    "#Zero out the weights corresponding to this class\n",
    "for task_id in range(len(task_labels)):\n",
    "    set_active_outputs(task_labels[task_id])\n",
    "    \n",
    "    for k,layer in enumerate(model.layers):  # Exclude output layer\n",
    "        if isinstance(layer, Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            if layer == model.layers[-1]:\n",
    "                for cl in class_to_forget:\n",
    "                    weights[:, cl] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "                    biases[cl] = 0\n",
    "            else:\n",
    "                for iids in weight_indices_zero[forget_task][k]:\n",
    "                    biases[iids[0]] = 0\n",
    "                for iids in weight_indices_zero[forget_task][k+2]:\n",
    "                    biases[iids[0]] = 0\n",
    "                    \n",
    "            layer.set_weights([weights, biases])\n",
    "    \n",
    "    print('Task ', {task_id}, ' accuracy: ', masked_predict(model, validation_datasets[task_id][0], validation_datasets[task_id][1]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-id for last epoch of each task: [64, 129, 189, 254, 314]\n",
      "*----------------------------*\n",
      "Loss after task 0  0.009885119\n",
      "Loss after task 1  0.08796781\n",
      "Loss after task 2  0.09093419\n",
      "Loss after task 3  0.04081106\n",
      "Loss after task 4  0.113942795\n",
      "*----------------------------*\n",
      "Surrogate loss after task 0  0.0\n",
      "Surrogate loss after task 1  0.037397042\n",
      "Surrogate loss after task 2  0.06461409\n",
      "Surrogate loss after task 3  0.034605086\n",
      "Surrogate loss after task 4  0.029006097\n"
     ]
    }
   ],
   "source": [
    "#Inspection of losses after every task \n",
    "loss_after_task=[]\n",
    "print('Batch-id for last epoch of each task:',last_epoch_batchindex)\n",
    "print('*----------------------------*')\n",
    "for i,epoch_id in enumerate(last_epoch_batchindex):\n",
    "    loss_after_task.append(history.losses[epoch_id])\n",
    "    print('Loss after task {0} '.format(i), history.losses[epoch_id]) \n",
    "print('*----------------------------*')\n",
    "\n",
    "#Inspection of surrogate loss or regularization (sum(big_omega x (theta'-theta))) after every task \n",
    "surrogate_loss_after_task=[]\n",
    "for i,epoch_id in enumerate(last_epoch_batchindex):\n",
    "    surrogate_loss_after_task.append(history.regs[epoch_id])\n",
    "    print('Surrogate loss after task {0} '.format(i), history.regs[epoch_id]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
