{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e518ad3",
   "metadata": {},
   "source": [
    "#### Importing libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5a4b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, SimpleRNN\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "# resize and normalize\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size * image_size])\n",
    "x_test = np.reshape(x_test,[-1, image_size* image_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47df536",
   "metadata": {},
   "source": [
    "#### Creating tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca3d2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 size: Trainset - (12665, 784), (12665, 10), Testset - (2115, 784), (2115, 10)\n",
      "Task 1 size: Trainset - (12089, 784), (12089, 10), Testset - (2042, 784), (2042, 10)\n",
      "Task 2 size: Trainset - (11263, 784), (11263, 10), Testset - (1874, 784), (1874, 10)\n",
      "Task 3 size: Trainset - (12183, 784), (12183, 10), Testset - (1986, 784), (1986, 10)\n",
      "Task 4 size: Trainset - (11800, 784), (11800, 10), Testset - (1983, 784), (1983, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "np.random.seed(100)\n",
    "n_tasks = 5\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "task_labels = [[0,1], [2,3], [4,5], [6,7], [8,9]]\n",
    "#task_labels = [[8,9], [6,7], [4,5], [3,2], [1,0]]\n",
    "#task_labels = [[0,1], [4,5], [6,7], [8,9], [2,3]]\n",
    "#task_labels = [[4,2], [0,6], [3,8], [9,7], [1,5],[8,9],[6,7],[5,5],[3,2],[0,1]]\n",
    "#task_labels = [[16,14], [25, 7], [20, 27], [15, 1], [32, 19]]\n",
    "#task_labels = [[0,9], [7,8], [3,6], [1,4], [2,5]]\n",
    "#task_labels = [[9, 3], [1, 8], [7, 4], [0, 5], [6, 2]]\n",
    "#task_labels = [[0,1], [2,3,1,0],[4,5,1,2], [6,7,3,0],[8,9,4,6]]\n",
    "n_tasks = len(task_labels)\n",
    "nb_classes  = 10\n",
    "training_datasets = []\n",
    "validation_datasets = []\n",
    "multihead=False\n",
    "\n",
    "for labels in task_labels:\n",
    "    idx = np.in1d(y_train, labels)\n",
    "    if multihead:\n",
    "        label_map = np.arange(nb_classes)\n",
    "        label_map[labels] = np.arange(len(labels))\n",
    "        data = x_train[idx], np_utils.to_categorical(label_map[y_train[idx]], len(labels))\n",
    "    else:\n",
    "        data = x_train[idx], np_utils.to_categorical(y_train[idx], nb_classes)\n",
    "        training_datasets.append(data)\n",
    "\n",
    "for labels in task_labels:\n",
    "    idx = np.in1d(y_test, labels)\n",
    "    if multihead:\n",
    "        label_map = np.arange(nb_classes)\n",
    "        label_map[labels] = np.arange(len(labels))\n",
    "        data = x_test[idx], np_utils.to_categorical(label_map[y_test[idx]], len(labels))\n",
    "    else:\n",
    "        data = x_test[idx], np_utils.to_categorical(y_test[idx], nb_classes)\n",
    "        validation_datasets.append(data)\n",
    "        \n",
    "tasks_train={}; labels_train = {}; tasks_test = {}; labels_test = {}\n",
    "\n",
    "for i in range(len(task_labels)):\n",
    "    tasks_train[str(i)] = training_datasets[i][0]\n",
    "    labels_train[str(i)] = training_datasets[i][1]\n",
    "    tasks_test[str(i)] = validation_datasets[i][0]\n",
    "    labels_test[str(i)] = validation_datasets[i][1]\n",
    "    print('Task {0} size: Trainset - {1}, {2}, Testset - {3}, {4}'.format(i,tasks_train[str(i)].shape, labels_train[str(i)].shape, tasks_test[str(i)].shape, labels_test[str(i)].shape))\n",
    "\n",
    "Tasks_dumped = []\n",
    "for i in range(len(task_labels)):\n",
    "    Tasks_dumped.append((tasks_train[str(i)], labels_train[str(i)], tasks_test[str(i)], labels_test[str(i)], tasks_test[str(i)], labels_test[str(i)]))\n",
    "f = open('mnist_tasks.pkl', \"wb\")\n",
    "pickle.dump(Tasks_dumped, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce1b8b",
   "metadata": {},
   "source": [
    "#### Training the progressive learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d314b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Learning task 1: classification between labels [0, 1].----\n",
      "67/67 - 0s - loss: 0.0058 - accuracy: 0.9995 - 48ms/epoch - 724us/step\n",
      "----Learning task 2: classification between labels [2, 3].----\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.\n",
      "64/64 - 0s - loss: 0.0212 - accuracy: 0.9922 - 79ms/epoch - 1ms/step\n",
      "----Learning task 3: classification between labels [4, 5].----\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.\n",
      "59/59 - 0s - loss: 0.0199 - accuracy: 0.9957 - 81ms/epoch - 1ms/step\n",
      "----Learning task 4: classification between labels [6, 7].----\n",
      "63/63 - 0s - loss: 0.0189 - accuracy: 0.9980 - 76ms/epoch - 1ms/step\n",
      "----Learning task 5: classification between labels [8, 9].----\n",
      "62/62 - 0s - loss: 0.0616 - accuracy: 0.9834 - 79ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "def create_model(learning_rate, dense_1, dense_2):\n",
    "    assert learning_rate > 0 and dense_1 > 0 and dense_2 > 0, \"Did you set the right configuration?\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(dense_1), input_shape=(784,), activation='relu', name='fc1'))\n",
    "    model.add(Dense(int(dense_2), activation='relu', name='fc2'))\n",
    "    model.add(Dense(10, activation='softmax', name='output'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "import pickle\n",
    "import pdb\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.models import save_model\n",
    "from keras.models import load_model\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        last_model_stats = Model_Perf_save\n",
    "        for i,lay in enumerate(model.layers):\n",
    "            last_model_size = last_model_stats['shape'][-1][2*i+1][0]\n",
    "            layer_weights = lay.get_weights()\n",
    "            layer_weights[0][:last_model_stats['weights'][-1][2*i].shape[0],:last_model_size] = last_model_stats['weights'][-1][2*i]\n",
    "            layer_weights[1][:last_model_size] = last_model_stats['weights'][-1][2*i+1]\n",
    "            model.layers[i].set_weights(layer_weights)      \n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        last_model_stats = Model_Perf_save\n",
    "        for i,lay in enumerate(model.layers):\n",
    "            last_model_size = last_model_stats['shape'][-1][2*i+1][0]\n",
    "            layer_weights = lay.get_weights()\n",
    "            #pdb.set_trace()\n",
    "            layer_weights[0][:last_model_stats['weights'][-1][2*i].shape[0],:last_model_size] = last_model_stats['weights'][-1][2*i]\n",
    "            layer_weights[1][:last_model_size] = last_model_stats['weights'][-1][2*i+1]\n",
    "            model.layers[i].set_weights(layer_weights)\n",
    "        \n",
    "def create_task(data_path):\n",
    "    data = pickle.load(open(data_path, \"rb\"))\n",
    "    return data\n",
    "\n",
    "task_list = create_task('mnist_tasks.pkl')\n",
    "num_tasks=5\n",
    "Model_Perf_save = {}\n",
    "Model_Perf_save['acc'] = []\n",
    "Model_Perf_save['shape'] = []\n",
    "Model_Perf_save['weights'] = []\n",
    "for task_id in range(num_tasks):\n",
    "    print(f'----Learning task {task_id+1}: classification between labels {task_labels[task_id]}.----')\n",
    "    f = open('task_dataset.pkl', 'wb')\n",
    "    pickle.dump(task_list[task_id], f)\n",
    "    f.close()\n",
    "    if task_id == 0:\n",
    "        model = create_model(learning_rate=0.01, dense_1=50, dense_2=50)\n",
    "        #call one of the search algorithm\n",
    "        history = model.fit(task_list[task_id][0], task_list[task_id][1],\n",
    "              batch_size=128, epochs=10, verbose=0,\n",
    "              validation_data=(task_list[task_id][2], task_list[task_id][3]))\n",
    "    else:\n",
    "        dense_1 = 50 + Model_Perf_save['shape'][-1][1][0]\n",
    "        dense_2 = 50 + Model_Perf_save['shape'][-1][3][0]\n",
    "        model = create_model(learning_rate=0.01, dense_1=dense_1, dense_2=dense_2)\n",
    "        history = model.fit(task_list[task_id][0], task_list[task_id][1],\n",
    "              batch_size=128, epochs=10, verbose=0,\n",
    "              validation_data=(task_list[task_id][2], task_list[task_id][3]), callbacks  = [LossHistory()])\n",
    "    save_model(model, f'tasks_{task_id+1}_model.h5')\n",
    "    loss_and_metrics = model.evaluate(task_list[task_id][4], task_list[task_id][5], verbose=2)\n",
    "    Model_Perf_save['acc'].append(loss_and_metrics[1])\n",
    "    Model_Perf_save['shape'].append([i.shape for i in model.get_weights()])\n",
    "    Model_Perf_save['weights'].append(model.get_weights()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d552760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9995272159576416,\n",
       " 0.9921645522117615,\n",
       " 0.9957310557365417,\n",
       " 0.9979858994483948,\n",
       " 0.9833585619926453]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Perf_save['acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c297292",
   "metadata": {},
   "source": [
    "#### Verify above accuracy from the loded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deaa8ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy: 0.9995272159576416\n",
      "Task 2 accuracy: 0.9921645522117615\n",
      "Task 3 accuracy: 0.9957310557365417\n",
      "Task 4 accuracy: 0.9979858994483948\n",
      "Task 5 accuracy: 0.9833585619926453\n"
     ]
    }
   ],
   "source": [
    "for task_id in range(num_tasks):\n",
    "    # Load the model\n",
    "    loaded_model = load_model(f'tasks_{task_id+1}_model.h5')\n",
    "    print(f'Task {task_id+1} accuracy: {loaded_model.evaluate(task_list[task_id][4], task_list[task_id][5], verbose=0)[1]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953f210",
   "metadata": {},
   "source": [
    "#### Forgetting task 2 : task that does classification between [2,3].\n",
    "\n",
    "#### Zeroing weights only at output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70ae280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy: 0.9995272159576416\n",
      "Task 2 accuracy: 0.5053868889808655\n",
      "Task 3 accuracy: 0.9957310557365417\n",
      "Task 4 accuracy: 0.9979858994483948\n",
      "Task 5 accuracy: 0.9833585619926453\n"
     ]
    }
   ],
   "source": [
    "forget_task=2\n",
    "class_to_forget = task_labels[forget_task-1]\n",
    "#Zero out the weights corresponding to this class\n",
    "for task_id in range(num_tasks):\n",
    "    loaded_model = load_model(f'tasks_{task_id+1}_model.h5')\n",
    "    for k,layer in enumerate(loaded_model.layers):  # Exclude output layer\n",
    "        if isinstance(layer, Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            if layer == loaded_model.layers[-1]:\n",
    "                for cl in class_to_forget:\n",
    "                    weights[:, cl] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "                    biases[cl] = 0\n",
    "            layer.set_weights([weights, biases])\n",
    "    print(f'Task {task_id+1} accuracy: {loaded_model.evaluate(task_list[task_id][4], task_list[task_id][5], verbose=0)[1]}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6061b",
   "metadata": {},
   "source": [
    "An accuracy rate of 50% in the forget task is considered complete forgetting. This occurs because it's a binary classification task, and when the output consistently remains zero, it effectively represents one valid outcome for 50% of the inference dataset. It's important to note that in each task, the classes are either 0 or 1, and resetting the weights to zero consistently results in an output value of zero, which is a valid outcome for one of the classes.\n",
    "\n",
    "\n",
    "It's worth mentioning that the accuracy for the forget task doesn't precisely reach 50% due to a partial misalignment in the dimensionality of the inference dataset between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ab3f8",
   "metadata": {},
   "source": [
    "#### Zeroing weights at the hidden layers as well as output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a073d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy: 0.9995272159576416\n",
      "Task 2 accuracy: 0.5053868889808655\n",
      "Task 3 accuracy: 0.9962646961212158\n",
      "Task 4 accuracy: 0.9919435977935791\n",
      "Task 5 accuracy: 0.9934442639350891\n"
     ]
    }
   ],
   "source": [
    "forget_task=2\n",
    "class_to_forget = task_labels[forget_task-1]\n",
    "#Zero out the weights corresponding to this class\n",
    "for task_id in range(num_tasks):\n",
    "    loaded_model = load_model(f'tasks_{task_id+1}_model.h5')\n",
    "    for k,layer in enumerate(loaded_model.layers):  # Exclude output layer\n",
    "        if isinstance(layer, Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            if layer == loaded_model.layers[-1]:\n",
    "                for cl in class_to_forget:\n",
    "                    weights[:, cl] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "                    biases[cl] = 0\n",
    "            else:\n",
    "                if task_id < forget_task-1:\n",
    "                    pass\n",
    "                else:\n",
    "                    zero_weight_location = Model_Perf_save['shape'][task_id-1][2*k]\n",
    "                    weights[zero_weight_location[0]-1:, zero_weight_location[1]-1:] = 0\n",
    "                    zero_bias_location = Model_Perf_save['shape'][task_id-1][2*k+1]\n",
    "                    biases[zero_bias_location[0]:] = 0\n",
    "            layer.set_weights([weights, biases])\n",
    "    print(f'Task {task_id+1} accuracy: {loaded_model.evaluate(task_list[task_id][4], task_list[task_id][5], verbose=0)[1]}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4955cf1",
   "metadata": {},
   "source": [
    "Impact is minimal on other tasks when zeroing the hidden layer weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc284f7",
   "metadata": {},
   "source": [
    "#### Forgetting task 3 : task that does classification between [4,5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0c3188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy: 0.9995272159576416\n",
      "Task 2 accuracy: 0.9921645522117615\n",
      "Task 3 accuracy: 0.5240128040313721\n",
      "Task 4 accuracy: 0.9919435977935791\n",
      "Task 5 accuracy: 0.9934442639350891\n"
     ]
    }
   ],
   "source": [
    "forget_task=3\n",
    "class_to_forget = task_labels[forget_task-1]\n",
    "#Zero out the weights corresponding to this class\n",
    "for task_id in range(num_tasks):\n",
    "    loaded_model = load_model(f'tasks_{task_id+1}_model.h5')\n",
    "    for k,layer in enumerate(loaded_model.layers):  # Exclude output layer\n",
    "        if isinstance(layer, Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            if layer == loaded_model.layers[-1]:\n",
    "                for cl in class_to_forget:\n",
    "                    weights[:, cl] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "                    biases[cl] = 0\n",
    "            else:\n",
    "                if task_id < forget_task-1:\n",
    "                    pass\n",
    "                else:\n",
    "                    zero_weight_location = Model_Perf_save['shape'][task_id-1][2*k]\n",
    "                    weights[zero_weight_location[0]-1:, zero_weight_location[1]-1:] = 0\n",
    "                    zero_bias_location = Model_Perf_save['shape'][task_id-1][2*k+1]\n",
    "                    biases[zero_bias_location[0]:] = 0\n",
    "            layer.set_weights([weights, biases])\n",
    "    print(f'Task {task_id+1} accuracy: {loaded_model.evaluate(task_list[task_id][4], task_list[task_id][5], verbose=0)[1]}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcfbeb",
   "metadata": {},
   "source": [
    "#### Forgetting task 4 : task that does classification between [6,7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "007752f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy: 0.9995272159576416\n",
      "Task 2 accuracy: 0.9921645522117615\n",
      "Task 3 accuracy: 0.9957310557365417\n",
      "Task 4 accuracy: 0.4823766350746155\n",
      "Task 5 accuracy: 0.9934442639350891\n"
     ]
    }
   ],
   "source": [
    "forget_task=4\n",
    "class_to_forget = task_labels[forget_task-1]\n",
    "#Zero out the weights corresponding to this class\n",
    "for task_id in range(num_tasks):\n",
    "    loaded_model = load_model(f'tasks_{task_id+1}_model.h5')\n",
    "    for k,layer in enumerate(loaded_model.layers):  # Exclude output layer\n",
    "        if isinstance(layer, Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            if layer == loaded_model.layers[-1]:\n",
    "                for cl in class_to_forget:\n",
    "                    weights[:, cl] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "                    biases[cl] = 0\n",
    "            else:\n",
    "                if task_id < forget_task-1:\n",
    "                    pass\n",
    "                else:\n",
    "                    zero_weight_location = Model_Perf_save['shape'][task_id-1][2*k]\n",
    "                    weights[zero_weight_location[0]-1:, zero_weight_location[1]-1:] = 0\n",
    "                    zero_bias_location = Model_Perf_save['shape'][task_id-1][2*k+1]\n",
    "                    biases[zero_bias_location[0]:] = 0\n",
    "            layer.set_weights([weights, biases])\n",
    "    print(f'Task {task_id+1} accuracy: {loaded_model.evaluate(task_list[task_id][4], task_list[task_id][5], verbose=0)[1]}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb098b2",
   "metadata": {},
   "source": [
    "#### Forgetting task 1 (very first task) : task that does classification between [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b67a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 accuracy: 0.46335697174072266\n",
      "Task 2 accuracy: 0.9921645522117615\n",
      "Task 3 accuracy: 0.9957310557365417\n",
      "Task 4 accuracy: 0.9979858994483948\n",
      "Task 5 accuracy: 0.9833585619926453\n"
     ]
    }
   ],
   "source": [
    "forget_task=1\n",
    "class_to_forget = task_labels[forget_task-1]\n",
    "#Zero out the weights corresponding to this class\n",
    "for task_id in range(num_tasks):\n",
    "    loaded_model = load_model(f'tasks_{task_id+1}_model.h5')\n",
    "    for k,layer in enumerate(loaded_model.layers):  # Exclude output layer\n",
    "        if isinstance(layer, Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            if layer == loaded_model.layers[-1]:\n",
    "                for cl in class_to_forget:\n",
    "                    weights[:, cl] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "                    biases[cl] = 0\n",
    "            else:\n",
    "                if task_id < forget_task-1:\n",
    "                    pass\n",
    "                else:\n",
    "                    if task_id == 0:\n",
    "                        zero_weight_location = Model_Perf_save['shape'][task_id][2*k]\n",
    "                        weights[:zero_weight_location[0]-1, :zero_weight_location[1]-1] = 0\n",
    "                        zero_bias_location = Model_Perf_save['shape'][task_id-1][2*k+1]\n",
    "                        biases[:zero_bias_location[0]] = 0\n",
    "                    else:\n",
    "                        pass\n",
    "            layer.set_weights([weights, biases])\n",
    "    print(f'Task {task_id+1} accuracy: {loaded_model.evaluate(task_list[task_id][4], task_list[task_id][5], verbose=0)[1]}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b64aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
