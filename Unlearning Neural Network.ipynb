{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e5b215",
   "metadata": {},
   "source": [
    "To achieve machine unlearning without retraining on the filtered dataset, we can modify the weights of the neural network model directly to forget about class 2. One way to do this is by adjusting the weights corresponding to class 2 to be closer to the weights corresponding to class 0 (or any other class we want the model to learn to resemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d3d856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2559 - accuracy: 0.9269 - val_loss: 0.1351 - val_accuracy: 0.9603\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1104 - accuracy: 0.9674 - val_loss: 0.0967 - val_accuracy: 0.9705\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0766 - accuracy: 0.9770 - val_loss: 0.0870 - val_accuracy: 0.9734\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0582 - accuracy: 0.9822 - val_loss: 0.0792 - val_accuracy: 0.9744\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0454 - accuracy: 0.9862 - val_loss: 0.0713 - val_accuracy: 0.9780\n",
      "313/313 [==============================] - 0s 951us/step - loss: 0.0713 - accuracy: 0.9780\n",
      "Accuracy on test data before unlearning class 2: 0.9779999852180481\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 972    0    1    1    1    1    2    1    1    0]\n",
      " [   1 1124    2    0    0    1    2    1    4    0]\n",
      " [  10    3 1010    1    0    0    2    4    2    0]\n",
      " [   2    0    3  998    0    4    0    1    2    0]\n",
      " [   2    0    5    0  954    1    3    2    2   13]\n",
      " [   4    2    0    5    1  869    4    2    3    2]\n",
      " [   5    3    3    1    6    3  934    0    3    0]\n",
      " [   2    2    9    1    3    0    0 1006    1    4]\n",
      " [  10    0    6    9    3    3    1    4  935    3]\n",
      " [   5    5    0    5    5    4    1    6    0  978]]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8914\n",
      "Accuracy on test data after unlearning class 2: 0.8913999795913696\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 972    0    0    1    1    1    2    1    2    0]\n",
      " [   1 1125    0    1    0    1    2    1    4    0]\n",
      " [  84  240  131  291   14    0   44  110  116    2]\n",
      " [   2    0    0  999    0    4    0    3    2    0]\n",
      " [   3    0    0    0  955    1    5    3    2   13]\n",
      " [   4    2    0    5    1  869    4    2    3    2]\n",
      " [   5    3    0    1    6    4  936    0    3    0]\n",
      " [   2    4    1    1    3    0    0 1011    2    4]\n",
      " [  10    0    1   10    3    3    2    4  938    3]\n",
      " [   5    5    0    5    5    4    1    6    0  978]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define and train a model on all classes\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data before unlearning class 2:\", accuracy)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Define the class to forget\n",
    "class_to_forget = 2\n",
    "\n",
    "# Modify the weights of the model to forget about class 2\n",
    "weights = model.layers[-1].get_weights()\n",
    "weights[0][:,class_to_forget].fill(0) # Replace the weights corresponding to class 2 with zeros\n",
    "weights[1][class_to_forget] =0 # Replace the bias corresponding to class 2 with zero\n",
    "model.layers[-1].set_weights(weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data after unlearning class 2:\", accuracy)\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1861260",
   "metadata": {},
   "source": [
    "In this code:\n",
    "1. We define and train a neural network model on all classes of the MNIST dataset.\n",
    "2. We identify the class we want to forget, which in this case is class 2.\n",
    "3. We modify the weights of the output layer corresponding to class 2 to be zeros, effectively \"forgetting\" about class 2.\n",
    "4. We evaluate the accuracy of the modified model on the test data to see how well it performs after unlearning class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b319b",
   "metadata": {},
   "source": [
    "Although it worked. but I don't think it is the correct mechansim because zeroing the weights for the forget class is only applied on theoutput layer. What about rest of the hidden layers of the network, it still has the weights corresponding to the forget class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8951b",
   "metadata": {},
   "source": [
    "I am thinking in terms of the progressive learning setup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd017",
   "metadata": {},
   "source": [
    "#### To perform weight pruning to minimize the influence of the forgotten class in all layers of the network, we can iterate through each layer of the model and adjust the weights accordingly. We will set the weights corresponding to the forgotten class to zero or reduce their magnitude.\n",
    "\n",
    "We can incorporate L1 or L2 regularization to encourage sparsity in the weights of the neural network, effectively reducing the influence of the specific class we want to forget. We use L2 in the below code.\n",
    "\n",
    "Here's how you can implement this in Python using TensorFlow/Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24be5cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6044 - accuracy: 0.8999 - val_loss: 0.4036 - val_accuracy: 0.9267\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3972 - accuracy: 0.9281 - val_loss: 0.3785 - val_accuracy: 0.9340\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3602 - accuracy: 0.9375 - val_loss: 0.3616 - val_accuracy: 0.9357\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3357 - accuracy: 0.9416 - val_loss: 0.3200 - val_accuracy: 0.9471\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3202 - accuracy: 0.9447 - val_loss: 0.3316 - val_accuracy: 0.9388\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.9388\n",
      "Accuracy on test data before unlearning class 2: 0.9387999773025513\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 953    0    0    3    0   11    5    2    3    3]\n",
      " [   0 1128    0    3    0    0    1    0    3    0]\n",
      " [  15    7  890   47    5    1    7   12   38   10]\n",
      " [   0    1    0  992    0    1    0    7    5    4]\n",
      " [   1    1    1    2  862    3    7    2    5   98]\n",
      " [   5    2    0   30    0  835    9    1    5    5]\n",
      " [  10    3    0    2    8    6  917    2   10    0]\n",
      " [   0    7    4   13    0    1    0  965   10   28]\n",
      " [   5    3    1   39    5   13    7    5  877   19]\n",
      " [   2    6    0   14    4    7    1    4    2  969]]\n",
      "313/313 [==============================] - 0s 983us/step - loss: 0.5192 - accuracy: 0.8614\n",
      "Accuracy on test data after weight pruning: 0.8614000082015991\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 956    0    0    2    0   11    4    2    2    3]\n",
      " [   0 1128    0    3    0    1    1    0    2    0]\n",
      " [  28   47  121  625    4    2   37   24  117   27]\n",
      " [   0    1    0  991    0    2    0    7    5    4]\n",
      " [   1    1    2    2  853    3    6    3    3  108]\n",
      " [   5    2    0   25    0  842    9    1    6    2]\n",
      " [   9    3    3    2    8    7  914    2   10    0]\n",
      " [   0    9    2   14    0    0    0  967   10   26]\n",
      " [   6    4    2   42    5   12    7    5  874   17]\n",
      " [   2    6    0   15    4    7    1    4    2  968]]\n"
     ]
    }
   ],
   "source": [
    "# Define and train a model on all classes\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data before unlearning class 2:\", accuracy)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "# Define the class to forget\n",
    "class_to_forget = 2\n",
    "\n",
    "# Set L2 regularization penalty for weights corresponding to the forgotten class\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, Dense):\n",
    "        weights, biases = layer.get_weights()\n",
    "        if layer == model.layers[-1]:\n",
    "            weights[:, class_to_forget] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "        else:\n",
    "            weights[:, class_to_forget] *= 0.01  # Apply L2 regularization penalty to weights in hidden layers\n",
    "        layer.set_weights([weights, biases])\n",
    "'''\n",
    "# Now do it at the output layer\n",
    "weights = model.layers[-1].get_weights()\n",
    "weights[0][:,class_to_forget].fill(0) # Replace the weights corresponding to class 2 with zeros\n",
    "weights[1][class_to_forget] =0 # Replace the bias corresponding to class 2 with zero\n",
    "model.layers[-1].set_weights(weights)\n",
    "'''\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data after weight pruning:\", accuracy)\n",
    "\n",
    "# Compute confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f415d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
