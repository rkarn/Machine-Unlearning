{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e5b215",
   "metadata": {},
   "source": [
    "To achieve machine unlearning without retraining on the filtered dataset, we can modify the weights of the neural network model directly to forget about class 2. One way to do this is by adjusting the weights corresponding to class 2 to be closer to the weights corresponding to class 0 (or any other class we want the model to learn to resemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d3d856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2559 - accuracy: 0.9269 - val_loss: 0.1351 - val_accuracy: 0.9603\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1104 - accuracy: 0.9674 - val_loss: 0.0967 - val_accuracy: 0.9705\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0766 - accuracy: 0.9770 - val_loss: 0.0870 - val_accuracy: 0.9734\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0582 - accuracy: 0.9822 - val_loss: 0.0792 - val_accuracy: 0.9744\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0454 - accuracy: 0.9862 - val_loss: 0.0713 - val_accuracy: 0.9780\n",
      "313/313 [==============================] - 0s 951us/step - loss: 0.0713 - accuracy: 0.9780\n",
      "Accuracy on test data before unlearning class 2: 0.9779999852180481\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 972    0    1    1    1    1    2    1    1    0]\n",
      " [   1 1124    2    0    0    1    2    1    4    0]\n",
      " [  10    3 1010    1    0    0    2    4    2    0]\n",
      " [   2    0    3  998    0    4    0    1    2    0]\n",
      " [   2    0    5    0  954    1    3    2    2   13]\n",
      " [   4    2    0    5    1  869    4    2    3    2]\n",
      " [   5    3    3    1    6    3  934    0    3    0]\n",
      " [   2    2    9    1    3    0    0 1006    1    4]\n",
      " [  10    0    6    9    3    3    1    4  935    3]\n",
      " [   5    5    0    5    5    4    1    6    0  978]]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8914\n",
      "Accuracy on test data after unlearning class 2: 0.8913999795913696\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 972    0    0    1    1    1    2    1    2    0]\n",
      " [   1 1125    0    1    0    1    2    1    4    0]\n",
      " [  84  240  131  291   14    0   44  110  116    2]\n",
      " [   2    0    0  999    0    4    0    3    2    0]\n",
      " [   3    0    0    0  955    1    5    3    2   13]\n",
      " [   4    2    0    5    1  869    4    2    3    2]\n",
      " [   5    3    0    1    6    4  936    0    3    0]\n",
      " [   2    4    1    1    3    0    0 1011    2    4]\n",
      " [  10    0    1   10    3    3    2    4  938    3]\n",
      " [   5    5    0    5    5    4    1    6    0  978]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define and train a model on all classes\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data before unlearning class 2:\", accuracy)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Define the class to forget\n",
    "class_to_forget = 2\n",
    "\n",
    "# Modify the weights of the model to forget about class 2\n",
    "weights = model.layers[-1].get_weights()\n",
    "weights[0][:,class_to_forget].fill(0) # Replace the weights corresponding to class 2 with zeros\n",
    "weights[1][class_to_forget] =0 # Replace the bias corresponding to class 2 with zero\n",
    "model.layers[-1].set_weights(weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data after unlearning class 2:\", accuracy)\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1861260",
   "metadata": {},
   "source": [
    "In this code:\n",
    "1. We define and train a neural network model on all classes of the MNIST dataset.\n",
    "2. We identify the class we want to forget, which in this case is class 2.\n",
    "3. We modify the weights of the output layer corresponding to class 2 to be zeros, effectively \"forgetting\" about class 2.\n",
    "4. We evaluate the accuracy of the modified model on the test data to see how well it performs after unlearning class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b319b",
   "metadata": {},
   "source": [
    "Although it worked. but I don't think it is the correct mechansim because zeroing the weights for the forget class is only applied on theoutput layer. What about rest of the hidden layers of the network, it still has the weights corresponding to the forget class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8951b",
   "metadata": {},
   "source": [
    "I am thinking in terms of the progressive learning setup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd017",
   "metadata": {},
   "source": [
    "#### To perform weight pruning to minimize the influence of the forgotten class in all layers of the network, we can iterate through each layer of the model and adjust the weights accordingly. We will set the weights corresponding to the forgotten class to zero or reduce their magnitude.\n",
    "\n",
    "We can incorporate L1 or L2 regularization to encourage sparsity in the weights of the neural network, effectively reducing the influence of the specific class we want to forget. We use L2 in the below code.\n",
    "\n",
    "Here's how you can implement this in Python using TensorFlow/Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24be5cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6044 - accuracy: 0.8999 - val_loss: 0.4036 - val_accuracy: 0.9267\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3972 - accuracy: 0.9281 - val_loss: 0.3785 - val_accuracy: 0.9340\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3602 - accuracy: 0.9375 - val_loss: 0.3616 - val_accuracy: 0.9357\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3357 - accuracy: 0.9416 - val_loss: 0.3200 - val_accuracy: 0.9471\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3202 - accuracy: 0.9447 - val_loss: 0.3316 - val_accuracy: 0.9388\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.9388\n",
      "Accuracy on test data before unlearning class 2: 0.9387999773025513\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 953    0    0    3    0   11    5    2    3    3]\n",
      " [   0 1128    0    3    0    0    1    0    3    0]\n",
      " [  15    7  890   47    5    1    7   12   38   10]\n",
      " [   0    1    0  992    0    1    0    7    5    4]\n",
      " [   1    1    1    2  862    3    7    2    5   98]\n",
      " [   5    2    0   30    0  835    9    1    5    5]\n",
      " [  10    3    0    2    8    6  917    2   10    0]\n",
      " [   0    7    4   13    0    1    0  965   10   28]\n",
      " [   5    3    1   39    5   13    7    5  877   19]\n",
      " [   2    6    0   14    4    7    1    4    2  969]]\n",
      "313/313 [==============================] - 0s 983us/step - loss: 0.5192 - accuracy: 0.8614\n",
      "Accuracy on test data after weight pruning: 0.8614000082015991\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 956    0    0    2    0   11    4    2    2    3]\n",
      " [   0 1128    0    3    0    1    1    0    2    0]\n",
      " [  28   47  121  625    4    2   37   24  117   27]\n",
      " [   0    1    0  991    0    2    0    7    5    4]\n",
      " [   1    1    2    2  853    3    6    3    3  108]\n",
      " [   5    2    0   25    0  842    9    1    6    2]\n",
      " [   9    3    3    2    8    7  914    2   10    0]\n",
      " [   0    9    2   14    0    0    0  967   10   26]\n",
      " [   6    4    2   42    5   12    7    5  874   17]\n",
      " [   2    6    0   15    4    7    1    4    2  968]]\n"
     ]
    }
   ],
   "source": [
    "# Define and train a model on all classes\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data before unlearning class 2:\", accuracy)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "# Define the class to forget\n",
    "class_to_forget = 2\n",
    "\n",
    "# Set L2 regularization penalty for weights corresponding to the forgotten class\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, Dense):\n",
    "        weights, biases = layer.get_weights()\n",
    "        if layer == model.layers[-1]:\n",
    "            weights[:, class_to_forget] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "        else:\n",
    "            weights[:, class_to_forget] *= 0.01  # Apply L2 regularization penalty to weights in hidden layers\n",
    "        layer.set_weights([weights, biases])\n",
    "'''\n",
    "# Now do it at the output layer\n",
    "weights = model.layers[-1].get_weights()\n",
    "weights[0][:,class_to_forget].fill(0) # Replace the weights corresponding to class 2 with zeros\n",
    "weights[1][class_to_forget] =0 # Replace the bias corresponding to class 2 with zero\n",
    "model.layers[-1].set_weights(weights)\n",
    "'''\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data after weight pruning:\", accuracy)\n",
    "\n",
    "# Compute confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0ece8",
   "metadata": {},
   "source": [
    "#### Sensitivity analysis for finding important weights.\n",
    "\n",
    "We define a neural network model with two hidden layers and an output layer.\n",
    "We train the model on the MNIST dataset.\n",
    "For each layer (excluding the output layer), we perturb a random sample of weights and measure the change in accuracy on the specified class.\n",
    "We print the top 10 most important weights for each layer based on sensitivity.\n",
    "\n",
    "We exclude the output layer because it is well known that the weights directed towards the forgetclass node are most important for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a9c6c0ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2345 - accuracy: 0.9305 - val_loss: 0.1173 - val_accuracy: 0.9653\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0991 - accuracy: 0.9697 - val_loss: 0.0997 - val_accuracy: 0.9669\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0694 - accuracy: 0.9782 - val_loss: 0.0857 - val_accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0531 - accuracy: 0.9829 - val_loss: 0.0722 - val_accuracy: 0.9783\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0408 - accuracy: 0.9860 - val_loss: 0.0821 - val_accuracy: 0.9757\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0821 - accuracy: 0.9757\n",
      "Baseline accuracy: 0.9757000207901001\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Confusion Matrix:\n",
      "[[ 971    0    1    1    1    0    2    2    2    0]\n",
      " [   0 1126    0    3    0    1    2    0    3    0]\n",
      " [   3    1  997   11    1    0    2    4   12    1]\n",
      " [   1    0    5  982    0    7    0    5    0   10]\n",
      " [   1    1    6    1  946    1    2    4    0   20]\n",
      " [   2    0    0    4    2  876    3    0    2    3]\n",
      " [   4    3    1    1    8    4  934    0    3    0]\n",
      " [   1    4   10    1    3    0    0 1000    1    8]\n",
      " [   9    1    3    4    0    7    2    4  939    5]\n",
      " [   4    5    0    1    6    2    0    5    0  986]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Define the class to analyze\n",
    "class_to_analyze = 2\n",
    "\n",
    "# Filter the dataset for the specified class\n",
    "X_train_class = X_train[y_train == class_to_analyze]\n",
    "X_test_class = X_test[y_test == class_to_analyze]\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Calculate the baseline accuracy\n",
    "baseline_loss, baseline_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Baseline accuracy:\", baseline_accuracy)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2d60547c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: dense_58\n",
      "Top 10 most important weights:\n",
      "Weight index: [[205  14]\n",
      " [693  83]\n",
      " [650  27]\n",
      " [654  70]\n",
      " [736   3]\n",
      " [181  72]\n",
      " [487  20]\n",
      " [ 33  16]\n",
      " [ 53  11]\n",
      " [663 127]] Sensitivity: [-0.00027519 -0.00027519 -0.00027519 -0.00027519 -0.00027519 -0.00027519\n",
      " -0.00027519 -0.00027519 -0.00027519 -0.00027519]\n",
      "Layer: dense_59\n",
      "Top 10 most important weights:\n",
      "Weight index: [[ 12  71]\n",
      " [ 26  39]\n",
      " [105  35]\n",
      " [103 123]\n",
      " [ 58   3]\n",
      " [ 81  98]\n",
      " [114 122]\n",
      " [  3  75]\n",
      " [ 37  46]\n",
      " [ 53   1]] Sensitivity: [-0.00027519 -0.00027519 -0.00027519 -0.00027519 -0.00027519 -0.00027519\n",
      " -0.00027519 -0.00027519 -0.00027519 -0.00027519]\n"
     ]
    }
   ],
   "source": [
    "# Define the fraction of weights to perturb\n",
    "fraction_to_perturb = 0.01  # 1% of the weights will be perturbed\n",
    "\n",
    "# Iterate through each layer and calculate sensitivity\n",
    "sensitivities = {}\n",
    "weights_location = {}\n",
    "for k,layer in enumerate(model.layers[:-1]):  # Exclude output layer\n",
    "    if hasattr(layer, 'weights') and len(layer.weights) > 0:\n",
    "        weights, biases = layer.get_weights()\n",
    "        num_weights_to_perturb = int(fraction_to_perturb * weights.size)\n",
    "        weight_indices_to_perturb = np.random.choice(weights.size, num_weights_to_perturb, replace=False)\n",
    "        sensitivities[k+1] = []\n",
    "        weights_location[k+1] = []\n",
    "        for idx in weight_indices_to_perturb:\n",
    "            i, j = np.unravel_index(idx, weights.shape)\n",
    "            # Temporarily set the weight to zero\n",
    "            original_weight = weights[i, j].copy()\n",
    "            weights[i, j] = 0\n",
    "\n",
    "            # Set the modified weights to the model\n",
    "            layer.set_weights([weights, biases])\n",
    "\n",
    "            # Evaluate the model with the modified weights\n",
    "            loss, accuracy = model.evaluate(X_test_class, np.full(len(X_test_class), class_to_analyze), verbose=0)\n",
    "\n",
    "            # Restore the original weight\n",
    "            weights[i, j] = original_weight\n",
    "\n",
    "            # Calculate the change in accuracy\n",
    "            sensitivity = baseline_accuracy - accuracy\n",
    "            sensitivities[k+1].append(sensitivity)\n",
    "            weights_location[k+1].append((i,j))\n",
    "        # Sort the sensitivities and get the indices of the most important weights\n",
    "        important_weight_indices = np.argsort(sensitivities[k+1])\n",
    "        print(f\"Layer: {layer.name}\")\n",
    "        print(\"Top 10 most important weights:\")\n",
    "        print(\"Weight index:\", np.array(weights_location[k+1])[important_weight_indices][:10] , \"Sensitivity:\", np.array(sensitivities[k+1])[important_weight_indices][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d7c6659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4205 - accuracy: 0.8829\n",
      "Accuracy on test data after weight pruning: 0.8828999996185303\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "Confusion Matrix:\n",
      "[[ 970    1    0    2    0    1    2    1    3    0]\n",
      " [   0 1115    0    6    0    1    2    2    9    0]\n",
      " [  38  218   63  313   35    0   38  311   15    1]\n",
      " [   2    0    0  996    0    2    0    5    1    4]\n",
      " [   0    1    1    1  960    1    7    1    2    8]\n",
      " [   2    0    0    6    0  876    4    0    3    1]\n",
      " [   4    2    0    2    4    4  942    0    0    0]\n",
      " [   1    0    0    9    0    1    0 1007    5    5]\n",
      " [  11    1    0   11    3    9    5    2  931    1]\n",
      " [   5    2    0    5    9   10    1    4    4  969]]\n"
     ]
    }
   ],
   "source": [
    "#Zero out the sensitive weights\n",
    "for k,layer in enumerate(model.layers):  # Exclude output layer\n",
    "    if isinstance(layer, Dense):\n",
    "        weights, biases = layer.get_weights()\n",
    "        if layer == model.layers[-1]:\n",
    "            weights[:, class_to_forget] = 0  # Zero out weights for the forgotten class in the output layer\n",
    "        else:\n",
    "            for (i,j) in weights_location[k+1][:30]: #zeroing 30 most sensitive weights\n",
    "                weights[i,j] = 0 #zero out sensitive weights\n",
    "        layer.set_weights([weights, biases])\n",
    "        \n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data after weight pruning:\", accuracy)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11753a7",
   "metadata": {},
   "source": [
    "#### This is showing some improvements than the regularization or the zeroing of output layer-only weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25049b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
