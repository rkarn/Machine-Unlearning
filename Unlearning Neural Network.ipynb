{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d86cc9f",
   "metadata": {},
   "source": [
    "To achieve machine unlearning without retraining on the filtered dataset, we can modify the weights of the neural network model directly to forget about class 2. One way to do this is by adjusting the weights corresponding to class 2 to be closer to the weights corresponding to class 0 (or any other class we want the model to learn to resemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b99d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2559 - accuracy: 0.9269 - val_loss: 0.1351 - val_accuracy: 0.9603\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1104 - accuracy: 0.9674 - val_loss: 0.0967 - val_accuracy: 0.9705\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0766 - accuracy: 0.9770 - val_loss: 0.0870 - val_accuracy: 0.9734\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0582 - accuracy: 0.9822 - val_loss: 0.0792 - val_accuracy: 0.9744\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0454 - accuracy: 0.9862 - val_loss: 0.0713 - val_accuracy: 0.9780\n",
      "313/313 [==============================] - 0s 951us/step - loss: 0.0713 - accuracy: 0.9780\n",
      "Accuracy on test data before unlearning class 2: 0.9779999852180481\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 972    0    1    1    1    1    2    1    1    0]\n",
      " [   1 1124    2    0    0    1    2    1    4    0]\n",
      " [  10    3 1010    1    0    0    2    4    2    0]\n",
      " [   2    0    3  998    0    4    0    1    2    0]\n",
      " [   2    0    5    0  954    1    3    2    2   13]\n",
      " [   4    2    0    5    1  869    4    2    3    2]\n",
      " [   5    3    3    1    6    3  934    0    3    0]\n",
      " [   2    2    9    1    3    0    0 1006    1    4]\n",
      " [  10    0    6    9    3    3    1    4  935    3]\n",
      " [   5    5    0    5    5    4    1    6    0  978]]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8914\n",
      "Accuracy on test data after unlearning class 2: 0.8913999795913696\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Confusion matrix:\n",
      "[[ 972    0    0    1    1    1    2    1    2    0]\n",
      " [   1 1125    0    1    0    1    2    1    4    0]\n",
      " [  84  240  131  291   14    0   44  110  116    2]\n",
      " [   2    0    0  999    0    4    0    3    2    0]\n",
      " [   3    0    0    0  955    1    5    3    2   13]\n",
      " [   4    2    0    5    1  869    4    2    3    2]\n",
      " [   5    3    0    1    6    4  936    0    3    0]\n",
      " [   2    4    1    1    3    0    0 1011    2    4]\n",
      " [  10    0    1   10    3    3    2    4  938    3]\n",
      " [   5    5    0    5    5    4    1    6    0  978]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define and train a model on all classes\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data before unlearning class 2:\", accuracy)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Define the class to forget\n",
    "class_to_forget = 2\n",
    "\n",
    "# Modify the weights of the model to forget about class 2\n",
    "weights = model.layers[-1].get_weights()\n",
    "weights[0][:,class_to_forget].fill(0) # Replace the weights corresponding to class 2 with zeros\n",
    "weights[1][class_to_forget] =0 # Replace the bias corresponding to class 2 with zero\n",
    "model.layers[-1].set_weights(weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data after unlearning class 2:\", accuracy)\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041175b",
   "metadata": {},
   "source": [
    "In this code:\n",
    "1. We define and train a neural network model on all classes of the MNIST dataset.\n",
    "2. We identify the class we want to forget, which in this case is class 2.\n",
    "3. We modify the weights of the output layer corresponding to class 2 to be zeros, effectively \"forgetting\" about class 2.\n",
    "4. We evaluate the accuracy of the modified model on the test data to see how well it performs after unlearning class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d89434",
   "metadata": {},
   "source": [
    "Although it worked. but I don't think it is the correct mechansim because zeroing the weights for the forget class is only applied on theoutput layer. What about rest of the hidden layers of the network, it still has the weights corresponding to the forget class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702eba6",
   "metadata": {},
   "source": [
    "I am thinking in terms of the progressive learning setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0592f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
